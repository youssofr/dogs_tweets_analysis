{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangling Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this document I will walk through the wrangling process; the steps I have taken in gathering, assessing and cleaning data. This describes the file wrangle_act.ipynb present in this directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering:\n",
    "\n",
    "I have gathered data from three different sources:\n",
    "\n",
    "1. A local csv file `'twitter-archive-enhanced.csv'` containing all basic tweets' data.\n",
    "    \n",
    "    \n",
    "2. A file hosted on [this link](https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv) containing predictions for images in some of the tweets of the first file, using [`requests`](https://requests.readthedocs.io/en/master/) python library.\n",
    "\n",
    "    First, I recieved the file in a `response` object then extracted the file to a text file, then read the file while taking care of tab dilemiter.\n",
    "\n",
    "\n",
    "3. Twitter API for additional information about the tweets. \n",
    "\n",
    "    I could not acquire a key from twitter. Yet, I have impleneted the piece of code that interacts with twitter api, sends a request and saves the resopnse to `recent_tweet_json.txt` in the case of a successful transaction. And in the case of a failed transaction, the error messages are stored in `twitter_api_log_file.txt` and the wrangling process continues on with `tweet_json.txt`, which was previously downloaded from course instruction page.\n",
    "\n",
    "    Checking if the download was successful or not is done by comparing the number of lines in each file: `recent_tweet_json.txt` and `tweet_json.txt` and choosing the recently downloaded one `recent_tweet_json.txt` if it has as much or more lines as the `tweet_json.txt`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing:\n",
    "\n",
    "Some problems where immidietly obvious on gathering the data, e.g. tidiness issues of the data about tweets being spread between three different sources and quality issues of unsuitable datatypes. Other prblems needed the usual visual inspection and programatic assessment.\n",
    "\n",
    "This step yielded the following problems:\n",
    "\n",
    "### Quality issues:\n",
    "1. `timestamp` and `retweeted_status_timestamp` in `df_tweets` is not formatted as `datetime`.\n",
    "1. all `id`'s: `['tweet_id', 'retweeted_status_id', 'retweeted_status_user_id', 'in_reply_to_status_id', 'in_reply_to_user_id']`  in `df_tweets` are should have the same `dtype`.\n",
    "1. Zero `rating_denominator` not allowed. In fact it was a correction. **(Correct the rating values)**\n",
    "1. Zero `rating_numerator` was not of a dog. **(Drop entry)**\n",
    "1. *50/50 split* mistaken for a rating. **(Change)**\n",
    "1. `Na` values in `df_tweets.name` has value `'None'` instead. **(Change to `Na`)**\n",
    "1. `'a'` is not a name. **(Change to `Na`)**\n",
    "1. `geo` column in `df_tweets_augment` has no values. **(Drop column)**\n",
    "1. make the variable created from aggregating `['doggo', 'floofer','pupper', 'puppo']` a `'category'`.\n",
    "\n",
    "### Tidiness issues:\n",
    "1. The `df_tweets_augment` and `df_tweets` both contain data about the same observational unit. **(Merge the two dataframes)**\n",
    "1. The `df_images` has predictions about images in tweets in `df_tweets`. So they are the same observational unit ()tweets. **(Merge the relevant pieces in `df_images` to `df_tweets`)**\n",
    "1. In `df_tweets` the columns:`['doggo', 'floofer','pupper', 'puppo']` form a list of values rather than variable names. **(`melt` within the frame)**\n",
    "\n",
    "These were all the issues I have began with, to later within the cleaning process dicover another issue with data quality.\n",
    "\n",
    "### Quality issue discovered while cleaning:\n",
    "\n",
    "10. Some tweets have, mistakenly, more than one dog stage at a time.\n",
    "\n",
    "Which I mended in the next step, the cleaning step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning:\n",
    "\n",
    "After identifying all the issues, I have then turned to cleaning the data. I have cleaned them one by one on each on three steps: \n",
    "\n",
    "1. ***Defining*** an approach to clean the issue.\n",
    "2. ***Coding*** my approach to obtain clean data.\n",
    "3. ***Testing*** if the cleaning code worked as expected and if the data is now as I intended it to be.\n",
    "\n",
    "I have first resolved quality issues in the main tweet archive. Then resolved tidiness issues and appended the relevant pieces of each dataframe together in one clean dataframe. Then resolved the final quality issue that emerged while cleaning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
